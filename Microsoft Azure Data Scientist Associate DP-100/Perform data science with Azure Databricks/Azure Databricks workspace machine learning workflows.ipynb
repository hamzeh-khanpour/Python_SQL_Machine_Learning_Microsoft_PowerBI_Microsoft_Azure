{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b432897-240f-4845-95bc-eb051034b203",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+--------------------+\n|            features|PaymentIsOutstanding|prediction|         probability|\n+--------------------+--------------------+----------+--------------------+\n|[22.0,1.0,3.0,53....|                   0|       0.0|[0.72474732443805...|\n|[22.0,1.0,3.0,53....|                   0|       0.0|[0.72474732443805...|\n|[22.0,1.0,3.0,53....|                   0|       0.0|[0.72474732443805...|\n|[22.0,1.0,3.0,53....|                   0|       0.0|[0.72474732443805...|\n|[22.0,1.0,3.0,53....|                   0|       0.0|[0.72474732443805...|\n+--------------------+--------------------+----------+--------------------+\nonly showing top 5 rows\nAUC Score: 0.5974475108716853\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# CHICAGO PARKING TICKETS - ML CLASSIFICATION\n",
    "# Predict PaymentIsOutstanding\n",
    "# ===============================================\n",
    "\n",
    "# 1️⃣ Imports\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# ===============================================\n",
    "# 2️⃣ Load Dataset\n",
    "# ===============================================\n",
    "\n",
    "file_path = \"/Volumes/hamzeh_databricks_workspace/default/hamzeh-volume/ChicagoParkingTickets.txt\"\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(file_path)\n",
    "\n",
    "# Remove duplicates\n",
    "df = df.dropDuplicates()\n",
    "\n",
    "# ===============================================\n",
    "# 3️⃣ Select Relevant Columns\n",
    "# ===============================================\n",
    "\n",
    "selected_cols = [\n",
    "    \"Community_Name\",\n",
    "    \"Sector\",\n",
    "    \"Side\",\n",
    "    \"Hardship_Index\",\n",
    "    \"Per_capita_income\",\n",
    "    \"Percent_unemployed\",\n",
    "    \"PaymentIsOutstanding\"\n",
    "]\n",
    "\n",
    "df = df.select(*selected_cols)\n",
    "\n",
    "# Drop rows with null target\n",
    "df = df.dropna(subset=[\"PaymentIsOutstanding\"])\n",
    "\n",
    "# Convert target to numeric (if needed)\n",
    "df = df.withColumn(\"PaymentIsOutstanding\",\n",
    "                   col(\"PaymentIsOutstanding\").cast(\"int\"))\n",
    "\n",
    "# ===============================================\n",
    "# 4️⃣ Feature Engineering\n",
    "# ===============================================\n",
    "\n",
    "# Encode categorical columns\n",
    "indexer1 = StringIndexer(\n",
    "    inputCol=\"Community_Name\",\n",
    "    outputCol=\"Community_index\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "indexer2 = StringIndexer(\n",
    "    inputCol=\"Sector\",\n",
    "    outputCol=\"Sector_index\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "indexer3 = StringIndexer(\n",
    "    inputCol=\"Side\",\n",
    "    outputCol=\"Side_index\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "# Combine all features into vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"Community_index\",\n",
    "        \"Sector_index\",\n",
    "        \"Side_index\",\n",
    "        \"Hardship_Index\",\n",
    "        \"Per_capita_income\",\n",
    "        \"Percent_unemployed\"\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# ===============================================\n",
    "# 5️⃣ Define Model\n",
    "# ===============================================\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"PaymentIsOutstanding\"\n",
    ")\n",
    "\n",
    "# ===============================================\n",
    "# 6️⃣ Build Pipeline\n",
    "# ===============================================\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    indexer1,\n",
    "    indexer2,\n",
    "    indexer3,\n",
    "    assembler,\n",
    "    lr\n",
    "])\n",
    "\n",
    "# ===============================================\n",
    "# 7️⃣ Train/Test Split\n",
    "# ===============================================\n",
    "\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# ===============================================\n",
    "# 8️⃣ Train Model\n",
    "# ===============================================\n",
    "\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "# ===============================================\n",
    "# 9️⃣ Predict\n",
    "# ===============================================\n",
    "\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "predictions.select(\n",
    "    \"features\",\n",
    "    \"PaymentIsOutstanding\",\n",
    "    \"prediction\",\n",
    "    \"probability\"\n",
    ").show(5)\n",
    "\n",
    "# ===============================================\n",
    "# \uD83D\uDD1F Evaluate Model\n",
    "# ===============================================\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"PaymentIsOutstanding\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "auc = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"AUC Score:\", auc)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2026-02-17 20:38:46",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}