{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ac60779-21f9-4289-a598-688347581c30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"/Volumes/hamzeh_databricks_workspace/default/hamzeh-volume/ChicagoParkingTickets.txt\"\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83214df4-2fa2-4940-8c5c-aafcb0ecd899",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+--------------------------------+--------------+--------------------+\n|Per_capita_income|Percent_unemployed|Percent_households_below_poverty|Hardship_Index|PaymentIsOutstanding|\n+-----------------+------------------+--------------------------------+--------------+--------------------+\n|          15957.0|              22.6|                            28.6|          73.0|                   1|\n|          18881.0|              24.0|                            27.8|          60.0|                   0|\n|          43198.0|               6.6|                            14.7|          10.0|                   0|\n|          15089.0|              13.1|                            20.5|          71.0|                   0|\n|          19713.0|              20.8|                            16.9|          48.0|                   0|\n+-----------------+------------------+--------------------------------+--------------+--------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "selected_cols = [\n",
    "    \"Per_capita_income\",\n",
    "    \"Percent_unemployed\",\n",
    "    \"Percent_households_below_poverty\",\n",
    "    \"Hardship_Index\",\n",
    "    \"PaymentIsOutstanding\"\n",
    "]\n",
    "\n",
    "df_ml = df.select(*selected_cols).dropna()\n",
    "df_ml.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db398940-72b8-433c-a74f-a0f7ee19fd7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_ml = df_ml.withColumn(\n",
    "    \"label\",\n",
    "    col(\"PaymentIsOutstanding\").cast(\"double\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de184412-12af-4db2-a9f5-7508173bead1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n|            features|label|\n+--------------------+-----+\n|[15957.0,22.6,28....|  1.0|\n|[18881.0,24.0,27....|  0.0|\n|[43198.0,6.6,14.7...|  0.0|\n|[15089.0,13.1,20....|  0.0|\n|[19713.0,20.8,16....|  0.0|\n+--------------------+-----+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature_cols = [\n",
    "    \"Per_capita_income\",\n",
    "    \"Percent_unemployed\",\n",
    "    \"Percent_households_below_poverty\",\n",
    "    \"Hardship_Index\"\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "df_final = assembler.transform(df_ml).select(\"features\", \"label\")\n",
    "df_final.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3329dcb0-94ce-4648-a305-437c6599b25b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = df_final.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d3feb40-8c21-4857-b4ac-1aee9e0a26b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "model = lr.fit(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3c2ffba-0d59-4dab-b488-faaee164b7d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+--------------------+\n|            features|label|prediction|         probability|\n+--------------------+-----+----------+--------------------+\n|[8201.0,34.6,56.5...|  0.0|       1.0|[0.49106088307408...|\n|[8201.0,34.6,56.5...|  0.0|       1.0|[0.49106088307408...|\n|[8201.0,34.6,56.5...|  0.0|       1.0|[0.49106088307408...|\n|[8201.0,34.6,56.5...|  0.0|       1.0|[0.49106088307408...|\n|[8201.0,34.6,56.5...|  0.0|       1.0|[0.49106088307408...|\n+--------------------+-----+----------+--------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test_df)\n",
    "predictions.select(\"features\", \"label\", \"prediction\", \"probability\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d392235-a5e5-4e31-b580-2030e703cf32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.5989228126831527\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(\"AUC:\", auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d2cc6a4-c4b8-4b3a-826d-ed4ea1e27804",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest AUC: 0.5914078003539687\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "rf_model = rf.fit(train_df)\n",
    "\n",
    "rf_predictions = rf_model.transform(test_df)\n",
    "\n",
    "auc_rf = evaluator.evaluate(rf_predictions)\n",
    "print(\"Random Forest AUC:\", auc_rf)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) Untitled Notebook 2026-02-17 19:50:16",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}