{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de8c7289-a66f-47e5-a6f2-da5730fed30e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+------------------+--------------+-----------------+------------------+-----------------------+--------------------------------+------------+----+------+-----+---------------+----------+-------------------+-------+------------+--------------------+\n|        Issued_date|    Community_Name|            Sector|              Side|Hardship_Index|Per_capita_income|Percent_unemployed|Percent_without_diploma|Percent_households_below_poverty|Neighborhood|Ward| Tract|  ZIP|Police_District|Plate_Type|License_Plate_State|Unit_ID|Violation_ID|PaymentIsOutstanding|\n+-------------------+------------------+------------------+------------------+--------------+-----------------+------------------+-----------------------+--------------------------------+------------+----+------+-----+---------------+----------+-------------------+-------+------------+--------------------+\n|2000-01-29 09:10:00|            Austin|      Other W Side|         West Side|          73.0|          15957.0|              22.6|                   24.4|                            28.6|          A3|  29|252101|60644|             15|       PAS|                 IL|     15|         123|                   1|\n|2014-08-21 09:31:00|           Chatham|Other S/SE/SW Side|Far Southeast Side|          60.0|          18881.0|              24.0|                   14.5|                            27.8|          C1|   8|440102|60619|              6|       PAS|                 IL|    498|           6|                   0|\n|2015-04-10 19:41:00|         West Town|         West Town|         West Side|          10.0|          43198.0|               6.6|                   12.9|                            14.7|         WP3|   1|241400|60622|             14|       PAS|                 IL|    502|          44|                   0|\n|2014-10-10 00:32:00|           Hermosa|   Other N/NW Side|    Northwest Side|          71.0|          15089.0|              13.1|                   41.6|                            20.5|          H2|  31|200100|60641|             25|       PAS|                 IL|    502|         163|                   0|\n|2015-12-19 15:46:00|Washington Heights|Other S/SE/SW Side|Far Southwest Side|          48.0|          19713.0|              20.8|                   13.7|                            16.9|          WH|  21|730201|60620|             22|       PAS|                 IL|     22|         169|                   0|\n+-------------------+------------------+------------------+------------------+--------------+-----------------+------------------+-----------------------+--------------------------------+------------+----+------+-----+---------------+----------+-------------------+-------+------------+--------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/Volumes/hamzeh_databricks_workspace/default/hamzeh-volume/ChicagoParkingTickets.txt\"\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(file_path)\n",
    "\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a577033b-53dc-4d9a-bf39-4fe621d72623",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"parking_tickets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb44cb2a-c8fe-4cb7-ac54-1a8752c30ad2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Issued_date: timestamp (nullable = true)\n |-- Community_Name: string (nullable = true)\n |-- Sector: string (nullable = true)\n |-- Side: string (nullable = true)\n |-- Hardship_Index: double (nullable = true)\n |-- Per_capita_income: double (nullable = true)\n |-- Percent_unemployed: double (nullable = true)\n |-- Percent_without_diploma: double (nullable = true)\n |-- Percent_households_below_poverty: double (nullable = true)\n |-- Neighborhood: string (nullable = true)\n |-- Ward: integer (nullable = true)\n |-- Tract: integer (nullable = true)\n |-- ZIP: integer (nullable = true)\n |-- Police_District: integer (nullable = true)\n |-- Plate_Type: string (nullable = true)\n |-- License_Plate_State: string (nullable = true)\n |-- Unit_ID: integer (nullable = true)\n |-- Violation_ID: integer (nullable = true)\n |-- PaymentIsOutstanding: integer (nullable = true)\n\n+---------------+-----+\n| Community_Name|count|\n+---------------+-----+\n|Near North Side|84386|\n|           Loop|68700|\n|      Lake View|66623|\n| Near West Side|60515|\n|   Lincoln Park|43163|\n|      West Town|42559|\n| South Lawndale|39436|\n|         Austin|34986|\n|   Logan Square|25023|\n|         Uptown|24914|\n+---------------+-----+\nonly showing top 10 rows\n+----+-----+\n|year|count|\n+----+-----+\n|1996|18314|\n|1997|21776|\n|1998|29800|\n|1999|30817|\n|2000|34040|\n|2001|55799|\n|2002|57816|\n|2003|57155|\n|2004|57853|\n|2005|53657|\n|2006|52774|\n|2007|54216|\n|2008|51784|\n|2009|51159|\n|2010|47080|\n|2011|45839|\n|2012|45177|\n|2013|47581|\n|2014|44807|\n|2015|44983|\n+----+-----+\nonly showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "\n",
    "df.count()\n",
    "\n",
    "df.groupBy(\"Community_Name\") \\\n",
    "  .count() \\\n",
    "  .orderBy(\"count\", ascending=False) \\\n",
    "  .show(10)\n",
    "\n",
    "from pyspark.sql.functions import year\n",
    "\n",
    "df.groupBy(year(\"Issued_date\").alias(\"year\")) \\\n",
    "  .count() \\\n",
    "  .orderBy(\"year\") \\\n",
    "  .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1ca6739-0952-4717-b7dd-6b077475b0fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+--------------------------------+--------------+--------------------+\n|Per_capita_income|Percent_unemployed|Percent_households_below_poverty|Hardship_Index|PaymentIsOutstanding|\n+-----------------+------------------+--------------------------------+--------------+--------------------+\n|          15957.0|              22.6|                            28.6|          73.0|                   1|\n|          18881.0|              24.0|                            27.8|          60.0|                   0|\n|          43198.0|               6.6|                            14.7|          10.0|                   0|\n|          15089.0|              13.1|                            20.5|          71.0|                   0|\n|          19713.0|              20.8|                            16.9|          48.0|                   0|\n+-----------------+------------------+--------------------------------+--------------+--------------------+\nonly showing top 5 rows\n+--------------------+-----+\n|            features|label|\n+--------------------+-----+\n|[15957.0,22.6,28....|  1.0|\n|[18881.0,24.0,27....|  0.0|\n|[43198.0,6.6,14.7...|  0.0|\n|[15089.0,13.1,20....|  0.0|\n|[19713.0,20.8,16....|  0.0|\n+--------------------+-----+\nonly showing top 5 rows\n+--------------------+-----+----------+--------------------+\n|            features|label|prediction|         probability|\n+--------------------+-----+----------+--------------------+\n|[8201.0,34.6,56.5...|  0.0|       1.0|[0.49106088307408...|\n|[8201.0,34.6,56.5...|  0.0|       1.0|[0.49106088307408...|\n|[8201.0,34.6,56.5...|  0.0|       1.0|[0.49106088307408...|\n|[8201.0,34.6,56.5...|  0.0|       1.0|[0.49106088307408...|\n|[8201.0,34.6,56.5...|  0.0|       1.0|[0.49106088307408...|\n+--------------------+-----+----------+--------------------+\nonly showing top 5 rows\nAUC: 0.5989228126831527\nRandom Forest AUC: 0.5914078003539687\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/Volumes/hamzeh_databricks_workspace/default/hamzeh-volume/ChicagoParkingTickets.txt\"\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(file_path)\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "selected_cols = [\n",
    "    \"Per_capita_income\",\n",
    "    \"Percent_unemployed\",\n",
    "    \"Percent_households_below_poverty\",\n",
    "    \"Hardship_Index\",\n",
    "    \"PaymentIsOutstanding\"\n",
    "]\n",
    "\n",
    "df_ml = df.select(*selected_cols).dropna()\n",
    "df_ml.show(5)\n",
    "\n",
    "\n",
    "\n",
    "df_ml = df_ml.withColumn(\n",
    "    \"label\",\n",
    "    col(\"PaymentIsOutstanding\").cast(\"double\")\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature_cols = [\n",
    "    \"Per_capita_income\",\n",
    "    \"Percent_unemployed\",\n",
    "    \"Percent_households_below_poverty\",\n",
    "    \"Hardship_Index\"\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "df_final = assembler.transform(df_ml).select(\"features\", \"label\")\n",
    "df_final.show(5)\n",
    "\n",
    "\n",
    "\n",
    "train_df, test_df = df_final.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "model = lr.fit(train_df)\n",
    "\n",
    "\n",
    "\n",
    "predictions = model.transform(test_df)\n",
    "predictions.select(\"features\", \"label\", \"prediction\", \"probability\").show(5)\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(\"AUC:\", auc)\n",
    "\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "rf_model = rf.fit(train_df)\n",
    "\n",
    "rf_predictions = rf_model.transform(test_df)\n",
    "\n",
    "auc_rf = evaluator.evaluate(rf_predictions)\n",
    "print(\"Random Forest AUC:\", auc_rf)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2026-02-17 19:24:50",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}